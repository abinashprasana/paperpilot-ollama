
# ğŸ“„ PaperPilot â€” Local Research Paper Assistant (Ollama)

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Streamlit](https://img.shields.io/badge/UI-Streamlit-red)
![LLM](https://img.shields.io/badge/LLM-Ollama-yellowgreen)
![Vector DB](https://img.shields.io/badge/VectorStore-FAISS-blueviolet)
![Status](https://img.shields.io/badge/Status-Working%20Prototype-brightgreen)

---

## ğŸ“Œ What this project is

**PaperPilot** is a simple, local research paper assistant that lets you:

- Upload a **single PDF document**
- Ask questions in **natural language**
- Get answers **strictly grounded in the uploaded document**

The goal of this project is to understand how **local LLMs** and a **basic Retrievalâ€‘Augmented Generation (RAG)** pipeline work together â€” without using any cloud APIs.

ğŸ‘‰ No OpenAI, no external services, and **no data leaves your machine**.

---

## ğŸ§  How it works (high level)

Think of the system as a small pipeline:

1. ğŸ“„ PDF is loaded and text is extracted  
2. âœ‚ï¸ Text is split into chunks  
3. ğŸ”¢ Embeddings are generated for each chunk  
4. ğŸ“š FAISS stores embeddings for fast similarity search  
5. ğŸ” Relevant chunks are retrieved for a question  
6. ğŸ¤– Ollama LLM answers using only those chunks  

This keeps answers **factâ€‘based** and tied to the document.

---

## ğŸ§© Project structure (explained)

```
paperpilot-ollama/
â”‚
â”œâ”€â”€ app.py            # Streamlit UI (entry point)
â”œâ”€â”€ pdf_loader.py     # Loads PDF and extracts raw text
â”œâ”€â”€ chunking.py       # Splits text into smaller chunks
â”œâ”€â”€ embeddings.py     # Converts chunks into vector embeddings
â”œâ”€â”€ vector_store.py   # Stores & searches embeddings using FAISS
â”œâ”€â”€ llm_client.py     # Communicates with local Ollama LLM
â”œâ”€â”€ qa.py             # Retrieval + answer generation logic
â”œâ”€â”€ prompts.py        # Prompt templates
â”œâ”€â”€ requirements.txt  # Python dependencies
â””â”€â”€ README.md         # Documentation
```

---

## ğŸ”— How everything connects

- `app.py` controls the flow and UI
- PDF â†’ chunking â†’ embeddings â†’ FAISS index
- User question â†’ similarity search â†’ retrieved chunks
- Chunks + prompt â†’ Ollama â†’ final answer

Each module has **one responsibility**, making the code easy to read and extend.

---

## ğŸ› ï¸ Technologies used

- Python 3.10+
- Streamlit
- Ollama (local LLM)
- FAISS (vector search)
- Retrievalâ€‘Augmented Generation (RAG)

---

## âš™ï¸ Setup

### Create virtual environment

```bash
python -m venv .venv
```

Activate:

- Windows:
```bash
.venv\Scripts\Activate.ps1
```

- macOS / Linux:
```bash
source .venv/bin/activate
```

### Install dependencies

```bash
pip install -r requirements.txt
```

### Start Ollama

```bash
ollama serve
ollama pull llama2
```

---

## ğŸš€ Run the app

```bash
streamlit run app.py
```

Open `http://localhost:8501` in your browser.

---

## ğŸ“ Notes / limitations

- This is a **learningâ€‘focused prototype**
- Best for textâ€‘based PDFs
- No user authentication or persistence
- Performance depends on document size

---

## ğŸ‘¤ Author

**Abinash Prasana Selvanathan**

---

â­ If you found this useful, consider starring the repo.
